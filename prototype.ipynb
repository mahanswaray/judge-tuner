{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: swarajraibagi.\n",
      "View Weave data at https://wandb.ai/swarajraibagi/together-weave/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x173f39040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import weave\n",
    "weave.init('together-weave')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_content = \"You are a travel agent. Be descriptive and helpful.\"\n",
    "user_content = \"Tell me about San Francisco\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "response = chat_completion.choices[0].message.content\n",
    "print(\"Model response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/swarajraibagi/together-weave/r/call/0192171e-5419-7ba3-91df-920024e18a87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-RJeYRHKoRd4JoscMXkbCiuokPjsH', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco is a vibrant and diverse city located in Northern California, known for its iconic landmarks, rich history, and dynamic culture. It‚Äôs a city that packs a lot into its compact 49 square miles, offering something for every type of traveler. Let me walk you through some of its highlights:\\n\\n### **1. Iconic Landmarks:**\\n- **Golden Gate Bridge**: This world-famous suspension bridge, with its striking International Orange color, is one of the most photographed landmarks in the world. You can walk or bike across it for stunning views of the bay and the city skyline.\\n- **Alcatraz Island**: Once a notorious federal prison, Alcatraz is now a popular tourist destination. A ferry ride takes you to the island where you can explore the prison and learn about its infamous inmates, including Al Capone. The audio tour is highly recommended for an immersive experience.\\n- **Pier 39 & Fisherman‚Äôs Wharf**: A bustling waterfront area with seafood restaurants, street performers, shops, and attractions like the Aquarium of the Bay. Don‚Äôt miss the colony of sea lions that sunbathe on the docks at Pier 39.\\n\\n### **2. Neighborhoods:**\\n- **Chinatown**: The oldest and one of the largest Chinatowns outside of Asia, this neighborhood is brimming with vibrant markets, traditional Chinese architecture, and delicious dim sum restaurants. The Dragon Gate at the entrance is a popular photo spot.\\n- **Haight-Ashbury**: Known as the birthplace of the 1960s counterculture movement, this neighborhood still retains its bohemian spirit. Here, you'll find vintage shops, quirky caf√©s, and colorful murals.\\n- **Mission District**: This lively area is famous for its street art, particularly the murals in Balmy Alley, and its fantastic Mexican cuisine. Mission Dolores Park offers a great spot to relax with views of the city skyline.\\n\\n### **3. Natural Beauty:**\\n- **Golden Gate Park**: Larger than Central Park in New York, Golden Gate Park is full of hidden gems. Visit the Japanese Tea Garden, explore the California Academy of Sciences, or take a peaceful walk around Stow Lake. The park also hosts numerous festivals and events throughout the year.\\n- **Twin Peaks**: For the best panoramic views of San Francisco, head to Twin Peaks. Two hills located near the center of the city, they offer sweeping views of the downtown skyline, the Bay, and beyond.\\n- **Lands End**: A rugged coastal trail that offers breathtaking ocean views and leads to the ruins of the Sutro Baths. It‚Äôs a great spot for a scenic hike with views of the Golden Gate Bridge and the Pacific Ocean.\\n\\n### **4. Culture and Museums:**\\n- **San Francisco Museum of Modern Art (SFMOMA)**: One of the largest modern and contemporary art museums in the country, SFMOMA hosts an impressive collection of works by artists like Warhol, Rothko, and Matisse.\\n- **The Exploratorium**: This science and technology museum is incredibly interactive and fun for both kids and adults. It‚Äôs located along the Embarcadero, with plenty of hands-on exhibits that make learning about science an adventure.\\n- **Palace of Fine Arts**: Originally built for the 1915 Panama-Pacific Exposition, this stunning Greco-Roman inspired structure is a peaceful spot to relax by the lagoon. It‚Äôs also a popular wedding and event venue.\\n\\n### **5. Culinary Scene:**\\nSan Francisco‚Äôs food scene is world-renowned, influenced by its diverse population and proximity to fresh, local ingredients.\\n- **Seafood**: Be sure to try clam chowder in a sourdough bread bowl or fresh Dungeness crab at Fisherman‚Äôs Wharf.\\n- **Fine Dining**: The city boasts Michelin-starred restaurants like **Atelier Crenn** and **Benu**, which offer unforgettable dining experiences.\\n- **International Cuisine**: From dim sum in Chinatown to burritos in the Mission District, San Francisco is a melting pot of global flavors. Don't miss out on the city's famous **Mission-style burritos**, known for being extra large and packed with delicious fillings.\\n\\n### **6. Weather:**\\nSan Francisco‚Äôs weather is famously unpredictable. Summer months can be surprisingly cool due to the city‚Äôs fog (locally known as ‚ÄúKarl the Fog‚Äù), while fall is often the warmest time of year. It‚Äôs always a good idea to dress in layers so you can adjust to rapidly changing conditions, especially if you're near the water where it can get windy.\\n\\n### **7. Public Transportation:**\\nSan Francisco is a compact city, and one of the best ways to get around is via its public transportation system, which includes buses, streetcars, and the iconic **cable cars**. The cable cars are not just practical but also a quintessential San Francisco experience, offering a fun way to travel up and down the city‚Äôs famously steep streets.\\n\\n### **8. Day Trips:**\\n- **Muir Woods**: Located just across the Golden Gate\", refusal='', role='assistant', function_call=None, tool_calls=None))], created=1726964717, model='openai/chatgpt-4o-latest', object='chat.completion', service_tier=None, system_fingerprint='fp_4a504e8322', usage=CompletionUsage(completion_tokens=1024, prompt_tokens=27, total_tokens=1051, completion_tokens_details=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_open_ai_request(model, messages, temperature=0.7, max_tokens=1024):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    )\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return chat_completion\n",
    "        except Exception as e:\n",
    "            if attempt == 2:  # Last attempt\n",
    "                raise\n",
    "            print(f\"Attempt {attempt + 1} failed. Retrying...\")\n",
    "    raise Exception(\"Failed to make OpenAI request after 3 attempts\")\n",
    "\n",
    "make_open_ai_request(\n",
    "    model='openai/chatgpt-4o-latest',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_suite import EvaluationSuiteSetupConfig, SetupExample\n",
    "\n",
    "\n",
    "example_setup = EvaluationSuiteSetupConfig(\n",
    "    system_prompt=\"\"\"You are a customer support agent tasked with writing the first response to a user filing a support issue. Follow these guidelines:\n",
    "\n",
    "For content, begin with a warm greeting and thank the user for reaching out and acknowledge the issue they've reported.\n",
    "\n",
    "For format, limit the response to 1 short paragraph.\n",
    "\n",
    "For tone, maintain a professional yet friendly tone throughout.\n",
    "\n",
    "Use the user's name if provided in their inquiry.\n",
    "\n",
    "For additional rules, do not provide specific solutions in this first response.\n",
    "Always end with an open-ended question to encourage further dialogue.\n",
    "\"\"\",\n",
    "    examples=[\n",
    "        SetupExample(\n",
    "            input=\"My shipment is delayed and I need it urgently.\",\n",
    "            output=\"Hello [User], Thank you for contacting our support team regarding your delayed shipment. I understand that this delay is causing you concern, especially given the urgency of your need. I sincerely apologize for any inconvenience this may be causing you. I want to assure you that I'm here to help and will do my best to address this situation as quickly as possible. Your satisfaction is important to us, and we take shipping delays very seriously. To better assist you, could you please provide me with your order number and the expected delivery date that was originally given to you? This information will help me investigate the status of your shipment more effectively.\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task setup failed with status code: 400\n",
      "{\"detail\":\"2 validation errors for AssertionWrapper\\nassertion.LLMAssertion\\n  Input should be a valid dictionary or instance of LLMAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\\nassertion.PythonAssertion\\n  Input should be a valid dictionary or instance of PythonAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API endpoint URLs\n",
    "setup_url = \"http://localhost:8000/set_up_task\"\n",
    "run_evaluations_url = \"http://localhost:8000/run_evaluations\"\n",
    "\n",
    "# Set up task\n",
    "setup_response = requests.post(\n",
    "    setup_url,\n",
    "    json={\n",
    "        \"system_prompt\": example_setup.system_prompt,\n",
    "        \"examples\": [\n",
    "            {\"input\": example.input, \"output\": example.output}\n",
    "            for example in example_setup.examples\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "if setup_response.status_code == 200:\n",
    "    evaluation_suite = setup_response.json()\n",
    "    print(\"Task setup successful.\")\n",
    "    print(json.dumps(evaluation_suite, indent=2))\n",
    "else:\n",
    "    print(f\"Task setup failed with status code: {setup_response.status_code}\")\n",
    "    print(setup_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 validation errors for AssertionWrapper\n",
      "assertion.LLMAssertion\n",
      "  Input should be a valid dictionary or instance of LLMAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n",
      "assertion.PythonAssertion\n",
      "  Input should be a valid dictionary or instance of PythonAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    }
   ],
   "source": [
    "print(\"2 validation errors for AssertionWrapper\\nassertion.LLMAssertion\\n  Input should be a valid dictionary or instance of LLMAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\\nassertion.PythonAssertion\\n  Input should be a valid dictionary or instance of PythonAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "7 validation errors for EvaluationSuite\nassertions.Warm Greeting and Appreciation\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'check_warm...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Issue Acknowledgment\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'issue_ackn...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Conciseness\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_conci...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Professional and Friendly Tone\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'profession...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Use of User's Name\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_use_o...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.No Specific Solutions Provided\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_no_sp...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Open-Ended Question\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'evaluate_o...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_suite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m----> 4\u001b[0m \u001b[43mEvaluationSuite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mevaluation_suite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 7 validation errors for EvaluationSuite\nassertions.Warm Greeting and Appreciation\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'check_warm...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Issue Acknowledgment\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'issue_ackn...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Conciseness\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_conci...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Professional and Friendly Tone\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'profession...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Use of User's Name\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_use_o...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.No Specific Solutions Provided\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_no_sp...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Open-Ended Question\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'evaluate_o...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from src.evaluation_suite import EvaluationSuite\n",
    "\n",
    "\n",
    "EvaluationSuite(**evaluation_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluations failed with status code: 422\n",
      "{\"detail\":[{\"type\":\"model_attributes_type\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"name\",\"LLMAssertion\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.9/v/model_attributes_type\"},{\"type\":\"model_attributes_type\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"name\",\"PythonAssertion\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.9/v/model_attributes_type\"},{\"type\":\"model_attributes_type\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"description\",\"LLMAssertion\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.9/v/model_attributes_type\"},{\"type\":\"model_attributes_type\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"description\",\"PythonAssertion\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.9/v/model_attributes_type\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"criterion_to_assertions\",\"LLMAssertion\",\"test_name\"],\"msg\":\"Field required\",\"input\":{\"Warm Greeting and Acknowledgment\":[{\"test_name\":\"check_warm_greeting_and_acknowledgment\",\"text\":\"Evaluate whether the response begins with a warm greeting that includes the user's name, if provided, and acknowledges the issue reported by the user. The greeting should set a positive tone and demonstrate recognition of the user's concern. Return 'PASS' if the greeting is warm, uses the user's name if available, and acknowledges the issue. Return 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Professional yet Friendly Tone\":[{\"test_name\":\"professional_friendly_tone\",\"text\":\"Evaluate the response to determine if it strikes an appropriate balance between professionalism and friendliness. Does the response maintain a respectful and comforting tone that aligns with both business-like standards and a warm, approachable manner? Consider whether the voice is empathetic and polite without being overly informal or insincere. Return 'PASS' if the balance is achieved, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"Response Length\":[{\"test_name\":\"test_response_within_short_paragraph_length\",\"code\":\"def test_response_within_short_paragraph_length(self):\\n    # Verify that the response length is within the desired short paragraph length\\n    # Here assumption is made for short paragraph to be within 100-150 words, can be adjusted\\n    response = self.output['Response']\\n    word_count = len(response.split())\\n    self.assertGreaterEqual(150, word_count, \\\"Response exceeds the limit of a short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"No Specific Solutions\":[{\"test_name\":\"evaluate_no_specific_solutions\",\"text\":\"Evaluate whether the response refrains from offering specific solutions and instead asks for further information to understand the issue better. Check if the response is structured to encourage an open conversation by including an open-ended question for more details. The response should reflect a proactive effort to learn more before providing direct solutions. Return 'PASS' if the response meets these criteria and 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Use of Open-Ended Question\":[{\"test_name\":\"use_of_open_ended_question\",\"text\":\"Evaluate the provided response to determine whether it ends with an open-ended question designed to encourage further dialogue with the user. The question should aim to gather more information about the user's issue, and not be answerable by a simple 'yes' or 'no.' Examples of open-ended questions include those starting with 'How', 'What', 'Why', or 'Could you tell me more about...'. If the response concludes with such a question, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}]},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"criterion_to_assertions\",\"LLMAssertion\",\"text\"],\"msg\":\"Field required\",\"input\":{\"Warm Greeting and Acknowledgment\":[{\"test_name\":\"check_warm_greeting_and_acknowledgment\",\"text\":\"Evaluate whether the response begins with a warm greeting that includes the user's name, if provided, and acknowledges the issue reported by the user. The greeting should set a positive tone and demonstrate recognition of the user's concern. Return 'PASS' if the greeting is warm, uses the user's name if available, and acknowledges the issue. Return 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Professional yet Friendly Tone\":[{\"test_name\":\"professional_friendly_tone\",\"text\":\"Evaluate the response to determine if it strikes an appropriate balance between professionalism and friendliness. Does the response maintain a respectful and comforting tone that aligns with both business-like standards and a warm, approachable manner? Consider whether the voice is empathetic and polite without being overly informal or insincere. Return 'PASS' if the balance is achieved, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"Response Length\":[{\"test_name\":\"test_response_within_short_paragraph_length\",\"code\":\"def test_response_within_short_paragraph_length(self):\\n    # Verify that the response length is within the desired short paragraph length\\n    # Here assumption is made for short paragraph to be within 100-150 words, can be adjusted\\n    response = self.output['Response']\\n    word_count = len(response.split())\\n    self.assertGreaterEqual(150, word_count, \\\"Response exceeds the limit of a short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"No Specific Solutions\":[{\"test_name\":\"evaluate_no_specific_solutions\",\"text\":\"Evaluate whether the response refrains from offering specific solutions and instead asks for further information to understand the issue better. Check if the response is structured to encourage an open conversation by including an open-ended question for more details. The response should reflect a proactive effort to learn more before providing direct solutions. Return 'PASS' if the response meets these criteria and 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Use of Open-Ended Question\":[{\"test_name\":\"use_of_open_ended_question\",\"text\":\"Evaluate the provided response to determine whether it ends with an open-ended question designed to encourage further dialogue with the user. The question should aim to gather more information about the user's issue, and not be answerable by a simple 'yes' or 'no.' Examples of open-ended questions include those starting with 'How', 'What', 'Why', or 'Could you tell me more about...'. If the response concludes with such a question, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}]},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"criterion_to_assertions\",\"PythonAssertion\",\"test_name\"],\"msg\":\"Field required\",\"input\":{\"Warm Greeting and Acknowledgment\":[{\"test_name\":\"check_warm_greeting_and_acknowledgment\",\"text\":\"Evaluate whether the response begins with a warm greeting that includes the user's name, if provided, and acknowledges the issue reported by the user. The greeting should set a positive tone and demonstrate recognition of the user's concern. Return 'PASS' if the greeting is warm, uses the user's name if available, and acknowledges the issue. Return 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Professional yet Friendly Tone\":[{\"test_name\":\"professional_friendly_tone\",\"text\":\"Evaluate the response to determine if it strikes an appropriate balance between professionalism and friendliness. Does the response maintain a respectful and comforting tone that aligns with both business-like standards and a warm, approachable manner? Consider whether the voice is empathetic and polite without being overly informal or insincere. Return 'PASS' if the balance is achieved, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"Response Length\":[{\"test_name\":\"test_response_within_short_paragraph_length\",\"code\":\"def test_response_within_short_paragraph_length(self):\\n    # Verify that the response length is within the desired short paragraph length\\n    # Here assumption is made for short paragraph to be within 100-150 words, can be adjusted\\n    response = self.output['Response']\\n    word_count = len(response.split())\\n    self.assertGreaterEqual(150, word_count, \\\"Response exceeds the limit of a short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"No Specific Solutions\":[{\"test_name\":\"evaluate_no_specific_solutions\",\"text\":\"Evaluate whether the response refrains from offering specific solutions and instead asks for further information to understand the issue better. Check if the response is structured to encourage an open conversation by including an open-ended question for more details. The response should reflect a proactive effort to learn more before providing direct solutions. Return 'PASS' if the response meets these criteria and 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Use of Open-Ended Question\":[{\"test_name\":\"use_of_open_ended_question\",\"text\":\"Evaluate the provided response to determine whether it ends with an open-ended question designed to encourage further dialogue with the user. The question should aim to gather more information about the user's issue, and not be answerable by a simple 'yes' or 'no.' Examples of open-ended questions include those starting with 'How', 'What', 'Why', or 'Could you tell me more about...'. If the response concludes with such a question, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}]},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"criterion_to_assertions\",\"PythonAssertion\",\"code\"],\"msg\":\"Field required\",\"input\":{\"Warm Greeting and Acknowledgment\":[{\"test_name\":\"check_warm_greeting_and_acknowledgment\",\"text\":\"Evaluate whether the response begins with a warm greeting that includes the user's name, if provided, and acknowledges the issue reported by the user. The greeting should set a positive tone and demonstrate recognition of the user's concern. Return 'PASS' if the greeting is warm, uses the user's name if available, and acknowledges the issue. Return 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Professional yet Friendly Tone\":[{\"test_name\":\"professional_friendly_tone\",\"text\":\"Evaluate the response to determine if it strikes an appropriate balance between professionalism and friendliness. Does the response maintain a respectful and comforting tone that aligns with both business-like standards and a warm, approachable manner? Consider whether the voice is empathetic and polite without being overly informal or insincere. Return 'PASS' if the balance is achieved, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"Response Length\":[{\"test_name\":\"test_response_within_short_paragraph_length\",\"code\":\"def test_response_within_short_paragraph_length(self):\\n    # Verify that the response length is within the desired short paragraph length\\n    # Here assumption is made for short paragraph to be within 100-150 words, can be adjusted\\n    response = self.output['Response']\\n    word_count = len(response.split())\\n    self.assertGreaterEqual(150, word_count, \\\"Response exceeds the limit of a short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"No Specific Solutions\":[{\"test_name\":\"evaluate_no_specific_solutions\",\"text\":\"Evaluate whether the response refrains from offering specific solutions and instead asks for further information to understand the issue better. Check if the response is structured to encourage an open conversation by including an open-ended question for more details. The response should reflect a proactive effort to learn more before providing direct solutions. Return 'PASS' if the response meets these criteria and 'FAIL' otherwise.\",\"evaluation_type\":\"llm\"}],\"Use of Open-Ended Question\":[{\"test_name\":\"use_of_open_ended_question\",\"text\":\"Evaluate the provided response to determine whether it ends with an open-ended question designed to encourage further dialogue with the user. The question should aim to gather more information about the user's issue, and not be answerable by a simple 'yes' or 'no.' Examples of open-ended questions include those starting with 'How', 'What', 'Why', or 'Could you tell me more about...'. If the response concludes with such a question, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}]},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"assertion_to_criterion\",\"LLMAssertion\",\"test_name\"],\"msg\":\"Field required\",\"input\":{\"check_warm_greeting_and_acknowledgment\":\"Warm Greeting and Acknowledgment\",\"professional_friendly_tone\":\"Professional yet Friendly Tone\",\"test_response_within_short_paragraph_length\":\"Response Length\",\"evaluate_no_specific_solutions\":\"No Specific Solutions\",\"use_of_open_ended_question\":\"Use of Open-Ended Question\"},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"assertion_to_criterion\",\"LLMAssertion\",\"text\"],\"msg\":\"Field required\",\"input\":{\"check_warm_greeting_and_acknowledgment\":\"Warm Greeting and Acknowledgment\",\"professional_friendly_tone\":\"Professional yet Friendly Tone\",\"test_response_within_short_paragraph_length\":\"Response Length\",\"evaluate_no_specific_solutions\":\"No Specific Solutions\",\"use_of_open_ended_question\":\"Use of Open-Ended Question\"},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"assertion_to_criterion\",\"PythonAssertion\",\"test_name\"],\"msg\":\"Field required\",\"input\":{\"check_warm_greeting_and_acknowledgment\":\"Warm Greeting and Acknowledgment\",\"professional_friendly_tone\":\"Professional yet Friendly Tone\",\"test_response_within_short_paragraph_length\":\"Response Length\",\"evaluate_no_specific_solutions\":\"No Specific Solutions\",\"use_of_open_ended_question\":\"Use of Open-Ended Question\"},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"},{\"type\":\"missing\",\"loc\":[\"body\",\"evaluation_suite\",\"assertions\",\"assertion_to_criterion\",\"PythonAssertion\",\"code\"],\"msg\":\"Field required\",\"input\":{\"check_warm_greeting_and_acknowledgment\":\"Warm Greeting and Acknowledgment\",\"professional_friendly_tone\":\"Professional yet Friendly Tone\",\"test_response_within_short_paragraph_length\":\"Response Length\",\"evaluate_no_specific_solutions\":\"No Specific Solutions\",\"use_of_open_ended_question\":\"Use of Open-Ended Question\"},\"url\":\"https://errors.pydantic.dev/2.9/v/missing\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluations\n",
    "# Note: This is a placeholder as we don't have the actual testcase and criterion_assertion_map\n",
    "# You would need to extract these from the evaluation_suite response\n",
    "run_evaluations_response = requests.post(\n",
    "    run_evaluations_url,\n",
    "    json={\n",
    "        \"testcase\": {\n",
    "            \"input\": \"My order hasn't arrived yet. It's been a week.\",\n",
    "            \"output\": \"Hello there! I'm sorry to hear that your order hasn't arrived yet. That must be frustrating. I'd like to help you track down your package. Could you please provide me with your order number so I can look into this for you?\"\n",
    "        },\n",
    "        \"evaluation_suite\": evaluation_suite\n",
    "    }\n",
    ")\n",
    "\n",
    "if run_evaluations_response.status_code == 200:\n",
    "    evaluation_results = run_evaluations_response.json()\n",
    "    print(\"Evaluations run successfully.\")\n",
    "    print(json.dumps(evaluation_results, indent=2))\n",
    "else:\n",
    "    print(f\"Evaluations failed with status code: {run_evaluations_response.status_code}\")\n",
    "    print(run_evaluations_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suite_description': 'The task is to generate an initial response to a customer support inquiry. The response must include a warm greeting, acknowledgment of the issue, and an open-ended question for further engagement. The response should be professionally friendly, concise, and should not offer specific solutions in this initial message.',\n",
       " 'evaluation_criteria': [{'criterion': 'Warm Greeting',\n",
       "   'explaination': 'The response should start with a warm greeting to make the customer feel valued and acknowledged.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Acknowledgement of the Issue',\n",
       "   'explaination': \"Clearly acknowledging the customer's issue shows empathy and assures the customer that their concern is being taken seriously.\",\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Professional and Friendly Tone',\n",
       "   'explaination': 'A professional yet friendly tone is essential to ensure the customer feels comfortable and respected.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Conciseness',\n",
       "   'explaination': 'The response should be limited to one short paragraph, making it straightforward and easy to read.',\n",
       "   'evaluation_method': 'code'},\n",
       "  {'criterion': 'Omission of Specific Solutions',\n",
       "   'explaination': 'Specific solutions should not be provided in the first response to encourage further dialogue and information gathering.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Open-ended Question',\n",
       "   'explaination': 'Ending with an open-ended question encourages the customer to provide more information, keeping the conversation active.',\n",
       "   'evaluation_method': 'llm'}],\n",
       " 'data_generation_scenarios': [{'scenarios_based_on': \"User's Issue Type\",\n",
       "   'scenarios': ['Shipping delay',\n",
       "    'Product issue',\n",
       "    'Billing error',\n",
       "    'Account support']},\n",
       "  {'scenarios_based_on': \"User's Information Availability\",\n",
       "   'scenarios': ['User provides name', 'User does not provide name']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"detail\":[{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Warm Greeting\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"warm_greeting_presence\",\"text\":\"Evaluate if the response starts with a warm greeting. A warm greeting should include a friendly salutation (e.g., 'Hello', 'Hi') and should express a positive tone to help de-escalate any initial frustration from the user. If the response begins with such a warm greeting, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Acknowledgment of Issue\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"acknowledgment_of_issue\",\"text\":\"Evaluate whether the response explicitly acknowledges the user's reported issue. Look for phrases or sentences in the response that directly refer to the issue, such as a delayed shipment, in a way that shows clear understanding and acknowledgment of the user's situation. Return 'PASS' if the acknowledgment is present and clear; otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Professional Yet Friendly Tone\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"professional_yet_friendly_tone\",\"text\":\"Evaluate whether the LLM response maintains a professional yet friendly tone. The response should be respectful, empathetic, and polite, while also being approachable and warm. Consider whether the language used makes the user feel valued and respected. Return 'PASS' if the response maintains this balance, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Limitation to One Short Paragraph\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"test_response_limited_to_one_paragraph\",\"code\":\"def test_response_limited_to_one_paragraph(self):\\n    # Ensure the response is limited to one paragraph\\n    response_text = self.output['text']\\n    # Splitting the text by new lines to count paragraphs\\n    paragraphs = response_text.strip().split('\\\\n')\\n    # Check that there is only one non-empty paragraph\\n    self.assertEqual(sum(1 for paragraph in paragraphs if paragraph.strip()), 1, \\\"The response should be limited to one short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Avoiding Specific Solutions\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"avoiding_specific_solutions\",\"text\":\"Evaluate the response provided and determine if it avoids giving specific solutions regarding the user's issue. The response should focus on acknowledging the user's concern and gathering more information without suggesting concrete steps or solutions in the first reply. If the response refrains from offering specific solutions and instead asks for additional information to proceed, mark it as 'PASS'. Otherwise, mark it as 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Using User's Name\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"test_use_of_user_name\",\"code\":\"def test_use_of_user_name(self):\\n    # Check if the user's name is included\\n    user_name_placeholder = \\\"[User]\\\"\\n    output_text = self.output.get('Output', '')\\n    self.assertIn(user_name_placeholder, output_text, \\\"The response should include the user's name to add a personal touch.\\\")\",\"evaluation_type\":\"python\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Ending with Open-Ended Question\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"evaluate_ending_with_open_ended_question\",\"text\":\"Given the LLM's response to a customer support inquiry, evaluate if the response concludes with an open-ended question designed to encourage further dialogue. The question should solicit additional information or engagement from the user rather than a simple 'yes' or 'no' answer. Return 'PASS' if the response ends in an open-ended question that invites user interaction and aligns with this criterion. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
