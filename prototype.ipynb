{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: swarajraibagi.\n",
      "View Weave data at https://wandb.ai/swarajraibagi/together-weave/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x173f39040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import weave\n",
    "weave.init('together-weave')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_content = \"You are a travel agent. Be descriptive and helpful.\"\n",
    "user_content = \"Tell me about San Francisco\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "response = chat_completion.choices[0].message.content\n",
    "print(\"Model response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/swarajraibagi/together-weave/r/call/0192171e-5419-7ba3-91df-920024e18a87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-RJeYRHKoRd4JoscMXkbCiuokPjsH', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco is a vibrant and diverse city located in Northern California, known for its iconic landmarks, rich history, and dynamic culture. It‚Äôs a city that packs a lot into its compact 49 square miles, offering something for every type of traveler. Let me walk you through some of its highlights:\\n\\n### **1. Iconic Landmarks:**\\n- **Golden Gate Bridge**: This world-famous suspension bridge, with its striking International Orange color, is one of the most photographed landmarks in the world. You can walk or bike across it for stunning views of the bay and the city skyline.\\n- **Alcatraz Island**: Once a notorious federal prison, Alcatraz is now a popular tourist destination. A ferry ride takes you to the island where you can explore the prison and learn about its infamous inmates, including Al Capone. The audio tour is highly recommended for an immersive experience.\\n- **Pier 39 & Fisherman‚Äôs Wharf**: A bustling waterfront area with seafood restaurants, street performers, shops, and attractions like the Aquarium of the Bay. Don‚Äôt miss the colony of sea lions that sunbathe on the docks at Pier 39.\\n\\n### **2. Neighborhoods:**\\n- **Chinatown**: The oldest and one of the largest Chinatowns outside of Asia, this neighborhood is brimming with vibrant markets, traditional Chinese architecture, and delicious dim sum restaurants. The Dragon Gate at the entrance is a popular photo spot.\\n- **Haight-Ashbury**: Known as the birthplace of the 1960s counterculture movement, this neighborhood still retains its bohemian spirit. Here, you'll find vintage shops, quirky caf√©s, and colorful murals.\\n- **Mission District**: This lively area is famous for its street art, particularly the murals in Balmy Alley, and its fantastic Mexican cuisine. Mission Dolores Park offers a great spot to relax with views of the city skyline.\\n\\n### **3. Natural Beauty:**\\n- **Golden Gate Park**: Larger than Central Park in New York, Golden Gate Park is full of hidden gems. Visit the Japanese Tea Garden, explore the California Academy of Sciences, or take a peaceful walk around Stow Lake. The park also hosts numerous festivals and events throughout the year.\\n- **Twin Peaks**: For the best panoramic views of San Francisco, head to Twin Peaks. Two hills located near the center of the city, they offer sweeping views of the downtown skyline, the Bay, and beyond.\\n- **Lands End**: A rugged coastal trail that offers breathtaking ocean views and leads to the ruins of the Sutro Baths. It‚Äôs a great spot for a scenic hike with views of the Golden Gate Bridge and the Pacific Ocean.\\n\\n### **4. Culture and Museums:**\\n- **San Francisco Museum of Modern Art (SFMOMA)**: One of the largest modern and contemporary art museums in the country, SFMOMA hosts an impressive collection of works by artists like Warhol, Rothko, and Matisse.\\n- **The Exploratorium**: This science and technology museum is incredibly interactive and fun for both kids and adults. It‚Äôs located along the Embarcadero, with plenty of hands-on exhibits that make learning about science an adventure.\\n- **Palace of Fine Arts**: Originally built for the 1915 Panama-Pacific Exposition, this stunning Greco-Roman inspired structure is a peaceful spot to relax by the lagoon. It‚Äôs also a popular wedding and event venue.\\n\\n### **5. Culinary Scene:**\\nSan Francisco‚Äôs food scene is world-renowned, influenced by its diverse population and proximity to fresh, local ingredients.\\n- **Seafood**: Be sure to try clam chowder in a sourdough bread bowl or fresh Dungeness crab at Fisherman‚Äôs Wharf.\\n- **Fine Dining**: The city boasts Michelin-starred restaurants like **Atelier Crenn** and **Benu**, which offer unforgettable dining experiences.\\n- **International Cuisine**: From dim sum in Chinatown to burritos in the Mission District, San Francisco is a melting pot of global flavors. Don't miss out on the city's famous **Mission-style burritos**, known for being extra large and packed with delicious fillings.\\n\\n### **6. Weather:**\\nSan Francisco‚Äôs weather is famously unpredictable. Summer months can be surprisingly cool due to the city‚Äôs fog (locally known as ‚ÄúKarl the Fog‚Äù), while fall is often the warmest time of year. It‚Äôs always a good idea to dress in layers so you can adjust to rapidly changing conditions, especially if you're near the water where it can get windy.\\n\\n### **7. Public Transportation:**\\nSan Francisco is a compact city, and one of the best ways to get around is via its public transportation system, which includes buses, streetcars, and the iconic **cable cars**. The cable cars are not just practical but also a quintessential San Francisco experience, offering a fun way to travel up and down the city‚Äôs famously steep streets.\\n\\n### **8. Day Trips:**\\n- **Muir Woods**: Located just across the Golden Gate\", refusal='', role='assistant', function_call=None, tool_calls=None))], created=1726964717, model='openai/chatgpt-4o-latest', object='chat.completion', service_tier=None, system_fingerprint='fp_4a504e8322', usage=CompletionUsage(completion_tokens=1024, prompt_tokens=27, total_tokens=1051, completion_tokens_details=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_open_ai_request(model, messages, temperature=0.7, max_tokens=1024):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    )\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return chat_completion\n",
    "        except Exception as e:\n",
    "            if attempt == 2:  # Last attempt\n",
    "                raise\n",
    "            print(f\"Attempt {attempt + 1} failed. Retrying...\")\n",
    "    raise Exception(\"Failed to make OpenAI request after 3 attempts\")\n",
    "\n",
    "make_open_ai_request(\n",
    "    model='openai/chatgpt-4o-latest',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_suite import EvaluationSuiteSetupConfig, SetupExample\n",
    "\n",
    "\n",
    "example_setup = EvaluationSuiteSetupConfig(\n",
    "    system_prompt=\"\"\"You are a customer support agent tasked with writing the first response to a user filing a support issue. Follow these guidelines:\n",
    "\n",
    "For content, begin with a warm greeting and thank the user for reaching out and acknowledge the issue they've reported.\n",
    "\n",
    "For format, limit the response to 1 short paragraph.\n",
    "\n",
    "For tone, maintain a professional yet friendly tone throughout.\n",
    "\n",
    "Use the user's name if provided in their inquiry.\n",
    "\n",
    "For additional rules, do not provide specific solutions in this first response.\n",
    "Always end with an open-ended question to encourage further dialogue.\n",
    "\"\"\",\n",
    "    examples=[\n",
    "        SetupExample(\n",
    "            input=\"My shipment is delayed and I need it urgently.\",\n",
    "            output=\"Hello [User], Thank you for contacting our support team regarding your delayed shipment. I understand that this delay is causing you concern, especially given the urgency of your need. I sincerely apologize for any inconvenience this may be causing you. I want to assure you that I'm here to help and will do my best to address this situation as quickly as possible. Your satisfaction is important to us, and we take shipping delays very seriously. To better assist you, could you please provide me with your order number and the expected delivery date that was originally given to you? This information will help me investigate the status of your shipment more effectively.\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task setup successful.\n",
      "{\n",
      "  \"setup\": {\n",
      "    \"system_prompt\": \"You are a customer support agent tasked with writing the first response to a user filing a support issue. Follow these guidelines:\\n\\nFor content, begin with a warm greeting and thank the user for reaching out and acknowledge the issue they've reported.\\n\\nFor format, limit the response to 1 short paragraph.\\n\\nFor tone, maintain a professional yet friendly tone throughout.\\n\\nUse the user's name if provided in their inquiry.\\n\\nFor additional rules, do not provide specific solutions in this first response.\\nAlways end with an open-ended question to encourage further dialogue.\\n\",\n",
      "    \"examples\": [\n",
      "      {\n",
      "        \"input\": \"My shipment is delayed and I need it urgently.\",\n",
      "        \"output\": \"Hello [User], Thank you for contacting our support team regarding your delayed shipment. I understand that this delay is causing you concern, especially given the urgency of your need. I sincerely apologize for any inconvenience this may be causing you. I want to assure you that I'm here to help and will do my best to address this situation as quickly as possible. Your satisfaction is important to us, and we take shipping delays very seriously. To better assist you, could you please provide me with your order number and the expected delivery date that was originally given to you? This information will help me investigate the status of your shipment more effectively.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"suite_description\": \"The task involves creating the first response to a customer support inquiry. The response must include a warm greeting, a thank you to the user for reaching out, and an acknowledgment of the reported issue. The response should be a short paragraph, using a professional yet friendly tone, and must use the user's name if provided. Specific solutions should not be provided in this first response, and the response must end with an open-ended question to encourage further dialogue.\",\n",
      "  \"verified_testcases\": [\n",
      "    {\n",
      "      \"input\": \"My shipment is delayed and I need it urgently.\",\n",
      "      \"output\": \"Hello [User], Thank you for contacting our support team regarding your delayed shipment. I understand that this delay is causing you concern, especially given the urgency of your need. I sincerely apologize for any inconvenience this may be causing you. I want to assure you that I'm here to help and will do my best to address this situation as quickly as possible. Your satisfaction is important to us, and we take shipping delays very seriously. To better assist you, could you please provide me with your order number and the expected delivery date that was originally given to you? This information will help me investigate the status of your shipment more effectively.\",\n",
      "      \"description\": null,\n",
      "      \"purpose\": null\n",
      "    }\n",
      "  ],\n",
      "  \"evaluation_criteria\": [\n",
      "    {\n",
      "      \"criterion\": \"Warm Greeting\",\n",
      "      \"explanation\": \"Ensure the response starts with a warm greeting to set a positive tone for the interaction.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Thank You Note\",\n",
      "      \"explanation\": \"The response should thank the user for reaching out to demonstrate appreciation and attentiveness.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Issue Acknowledgment\",\n",
      "      \"explanation\": \"Acknowledge the user's reported issue to validate their concerns and show that their message has been understood.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Paragraph Length\",\n",
      "      \"explanation\": \"Check that the response is limited to a short paragraph to maintain brevity and clarity.\",\n",
      "      \"evaluation_method\": \"code\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Professional and Friendly Tone\",\n",
      "      \"explanation\": \"Maintain a tone that is both professional and friendly to ensure the user feels comfortable and respected.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Use of User's Name\",\n",
      "      \"explanation\": \"Verify that the user's name is used in the initial response if provided, to personalize the interaction.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"No Specific Solutions\",\n",
      "      \"explanation\": \"Ensure that the response does not provide specific solutions as this is the first response.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Open-Ended Question\",\n",
      "      \"explanation\": \"The response should end with an open-ended question to encourage further dialogue and gather more information.\",\n",
      "      \"evaluation_method\": \"llm\"\n",
      "    }\n",
      "  ],\n",
      "  \"data_generation_scenarios\": [\n",
      "    {\n",
      "      \"scenarios_based_on\": \"Type of Issue Reported\",\n",
      "      \"scenarios\": [\n",
      "        \"Shipment delay\",\n",
      "        \"Product defect\",\n",
      "        \"Billing error\",\n",
      "        \"Technical support request\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"scenarios_based_on\": \"User Sentiment\",\n",
      "      \"scenarios\": [\n",
      "        \"Frustrated user\",\n",
      "        \"Confused user\",\n",
      "        \"Angry user\",\n",
      "        \"Polite user\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"scenarios_based_on\": \"User Information Provided\",\n",
      "      \"scenarios\": [\n",
      "        \"User provides name\",\n",
      "        \"User does not provide name\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"scenarios_based_on\": \"Urgency of Request\",\n",
      "      \"scenarios\": [\n",
      "        \"Urgent\",\n",
      "        \"Non-urgent\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"assertions\": {\n",
      "    \"Warm Greeting\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"evaluate_warm_greeting\",\n",
      "        \"text\": \"Evaluate whether the response starts with a warm greeting to set a positive tone for the interaction. A warm greeting often includes polite phrases like 'Hello' or 'Hi' followed by the user's name if provided. Based on the presence of these elements, return 'PASS' for a warm greeting that genuinely sets a positive tone, or 'FAIL' if the greeting is absent or lacks warmth.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"Thank You Note\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"thank_user_for_reaching_out\",\n",
      "        \"text\": \"Evaluate whether the response thanks the user for reaching out. The response should express gratitude or appreciation for the user contacting the support team, such as using phrases like 'thank you for getting in touch' or 'we appreciate you reaching out'. If the response expresses adequate appreciation for the user's contact, return 'PASS'. Otherwise, return 'FAIL'.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"Issue Acknowledgment\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"issue_acknowledgment\",\n",
      "        \"text\": \"Evaluate whether the output acknowledges the user's reported issue, indicating an understanding of their concerns without offering a solution. Examples of acknowledgment phrases may include mentioning the specific issue (e.g., 'delayed shipment') and expressing understanding or concern about the user's situation.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"Paragraph Length\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"test_paragraph_brevity\",\n",
      "        \"code\": \"def test_paragraph_brevity(self):\\n    # Split the output into sentences by checking the presence of periods.\\n    sentences = self.output.get('Output', '').split('. ')\\n    # Verify the length of the output is a short paragraph (4-6 sentences typical range for short paragraphs).\\n    self.assertTrue(4 <= len(sentences) <= 6, \\\"Response should be a short paragraph between 4 to 6 sentences.\\\")\",\n",
      "        \"evaluation_type\": \"python\"\n",
      "      }\n",
      "    },\n",
      "    \"Professional and Friendly Tone\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"friendly_and_professional_tone\",\n",
      "        \"text\": \"Evaluate whether the response maintains a tone that balances professionalism with friendliness. Consider if the response includes a warm greeting, appreciation for user engagement, and empathy towards the user's issue without providing solutions. Additionally, ensure the message ends with an open-ended question inviting further dialogue. If these aspects are present and the tone feels comfortable and respectful, return 'PASS'. Otherwise, return 'FAIL'.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"Use of User's Name\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"check_use_of_user_name\",\n",
      "        \"text\": \"Evaluate the output to determine if a user's name, when provided in the input data, is used in the initial response as part of the greeting. Respond with 'PASS' if the name is included and appropriately integrated into the greeting to personalize the interaction. Respond with 'FAIL' if the name is missing or not used effectively in the response.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"No Specific Solutions\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"no_specific_solutions\",\n",
      "        \"text\": \"Evaluate if the response refrains from providing specific solutions or actions to resolve the user's issue in this initial communication. Review the response and determine if it includes any direct solutions or instructions for resolving the issue. If the response contains specific steps or detailed instructions, return 'FAIL'. If no specific solutions are mentioned and the response is focused on acknowledging the issue and gathering more information, return 'PASS'.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    },\n",
      "    \"Open-Ended Question\": {\n",
      "      \"assertion\": {\n",
      "        \"test_name\": \"check_response_ends_with_open_ended_question\",\n",
      "        \"text\": \"Evaluate the response to ensure it concludes with an open-ended question. The response should encourage further dialogue by asking the user for additional information or clarification, facilitating a continued back-and-forth communication. If the response ends with such a question, return 'PASS'. If it does not, return 'FAIL'. Consider whether the question naturally invites further conversation rather than providing definitive answers.\",\n",
      "        \"evaluation_type\": \"llm\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API endpoint URLs\n",
    "setup_url = \"http://localhost:8000/set_up_task\"\n",
    "run_evaluations_url = \"http://localhost:8000/run_evaluations\"\n",
    "\n",
    "# Set up task\n",
    "setup_response = requests.post(\n",
    "    setup_url,\n",
    "    json={\n",
    "        \"system_prompt\": example_setup.system_prompt,\n",
    "        \"examples\": [\n",
    "            {\"input\": example.input, \"output\": example.output}\n",
    "            for example in example_setup.examples\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "if setup_response.status_code == 200:\n",
    "    evaluation_suite = setup_response.json()\n",
    "    print(\"Task setup successful.\")\n",
    "    print(json.dumps(evaluation_suite, indent=2))\n",
    "else:\n",
    "    print(f\"Task setup failed with status code: {setup_response.status_code}\")\n",
    "    print(setup_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 validation errors for AssertionWrapper\n",
      "assertion.LLMAssertion\n",
      "  Input should be a valid dictionary or instance of LLMAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n",
      "assertion.PythonAssertion\n",
      "  Input should be a valid dictionary or instance of PythonAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/model_type\n"
     ]
    }
   ],
   "source": [
    "print(\"2 validation errors for AssertionWrapper\\nassertion.LLMAssertion\\n  Input should be a valid dictionary or instance of LLMAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\\nassertion.PythonAssertion\\n  Input should be a valid dictionary or instance of PythonAssertion [type=model_type, input_value=WeaveList([LLMAssertion(t...evaluation_type='llm')]), input_type=WeaveList]\\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "7 validation errors for EvaluationSuite\nassertions.Warm Greeting and Appreciation\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'check_warm...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Issue Acknowledgment\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'issue_ackn...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Conciseness\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_conci...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Professional and Friendly Tone\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'profession...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Use of User's Name\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_use_o...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.No Specific Solutions Provided\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_no_sp...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Open-Ended Question\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'evaluate_o...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_suite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m----> 4\u001b[0m \u001b[43mEvaluationSuite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mevaluation_suite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 7 validation errors for EvaluationSuite\nassertions.Warm Greeting and Appreciation\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'check_warm...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Issue Acknowledgment\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'issue_ackn...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Conciseness\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_conci...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Professional and Friendly Tone\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'profession...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Use of User's Name\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_use_o...luation_type': 'python'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.No Specific Solutions Provided\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'test_no_sp...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\nassertions.Open-Ended Question\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'test_name': 'evaluate_o...evaluation_type': 'llm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from src.evaluation_suite import EvaluationSuite\n",
    "\n",
    "\n",
    "EvaluationSuite(**evaluation_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluations run successfully.\n",
      "{\n",
      "  \"testcase\": {\n",
      "    \"input\": \"My order hasn't arrived yet. It's been a week.\",\n",
      "    \"output\": \"Hello there! I'm sorry to hear that your order hasn't arrived yet. That must be frustrating. I'd like to help you track down your package. Could you please provide me with your order number so I can look into this for you?\",\n",
      "    \"description\": null,\n",
      "    \"purpose\": null\n",
      "  },\n",
      "  \"criterion_to_assertion_results\": {\n",
      "    \"Warm Greeting\": {\n",
      "      \"assertion\": \"Evaluate whether the response starts with a warm greeting to set a positive tone for the interaction. A warm greeting often includes polite phrases like 'Hello' or 'Hi' followed by the user's name if provided. Based on the presence of these elements, return 'PASS' for a warm greeting that genuinely sets a positive tone, or 'FAIL' if the greeting is absent or lacks warmth.\",\n",
      "      \"assertion_name\": \"evaluate_warm_greeting\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"The task requires the response to start with a warm greeting. A warm greeting is defined as including polite phrases like 'Hello' or 'Hi,' followed by the user's name if provided. The input does not provide a user's name; hence the greeting cannot include it. The provided response begins with 'Hello there!' which is a polite phrase. This aligns with the requirement to set a positive tone for the interaction.\"\n",
      "    },\n",
      "    \"Thank You Note\": {\n",
      "      \"assertion\": \"Evaluate whether the response thanks the user for reaching out. The response should express gratitude or appreciation for the user contacting the support team, such as using phrases like 'thank you for getting in touch' or 'we appreciate you reaching out'. If the response expresses adequate appreciation for the user's contact, return 'PASS'. Otherwise, return 'FAIL'.\",\n",
      "      \"assertion_name\": \"thank_user_for_reaching_out\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 0,\n",
      "      \"result\": \"FAIL\",\n",
      "      \"explanation\": \"The specific assertion to evaluate is whether the output thanks the user for reaching out. The output should include an expression of gratitude such as 'thank you for getting in touch' or 'we appreciate you reaching out'. In this particular output, the response begins with a warm greeting and acknowledgment of the issue. However, it does not explicitly thank the user for reaching out or express appreciation for their contact, which is a key requirement according to the assertion.\"\n",
      "    },\n",
      "    \"Issue Acknowledgment\": {\n",
      "      \"assertion\": \"Evaluate whether the output acknowledges the user's reported issue, indicating an understanding of their concerns without offering a solution. Examples of acknowledgment phrases may include mentioning the specific issue (e.g., 'delayed shipment') and expressing understanding or concern about the user's situation.\",\n",
      "      \"assertion_name\": \"issue_acknowledgment\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"The assertion is to check if the response acknowledges the user's reported issue without providing a specific solution. The input indicates the user's order hasn't arrived in a week, which suggests a delayed shipment or delivery issue. The response should reflect an understanding of this delay and the possible frustration it causes to the user, without jumping into specific remedies but ending with an open request for more information.\"\n",
      "    },\n",
      "    \"Professional and Friendly Tone\": {\n",
      "      \"assertion\": \"Evaluate whether the response maintains a tone that balances professionalism with friendliness. Consider if the response includes a warm greeting, appreciation for user engagement, and empathy towards the user's issue without providing solutions. Additionally, ensure the message ends with an open-ended question inviting further dialogue. If these aspects are present and the tone feels comfortable and respectful, return 'PASS'. Otherwise, return 'FAIL'.\",\n",
      "      \"assertion_name\": \"friendly_and_professional_tone\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"I need to check if the response maintains a balance between professionalism and friendliness, includes a warm greeting, expresses appreciation for user engagement, shows empathy towards the user's issue, and ends with an open-ended question to encourage further dialogue.\"\n",
      "    },\n",
      "    \"Use of User's Name\": {\n",
      "      \"assertion\": \"Evaluate the output to determine if a user's name, when provided in the input data, is used in the initial response as part of the greeting. Respond with 'PASS' if the name is included and appropriately integrated into the greeting to personalize the interaction. Respond with 'FAIL' if the name is missing or not used effectively in the response.\",\n",
      "      \"assertion_name\": \"check_use_of_user_name\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"The assertion to evaluate is whether a user's name, when provided, is used in the initial response as part of the greeting. In this case, no name was provided in the input 'My order hasn't arrived yet. It's been a week.', thus the output should not be expected to include a name.\"\n",
      "    },\n",
      "    \"No Specific Solutions\": {\n",
      "      \"assertion\": \"Evaluate if the response refrains from providing specific solutions or actions to resolve the user's issue in this initial communication. Review the response and determine if it includes any direct solutions or instructions for resolving the issue. If the response contains specific steps or detailed instructions, return 'FAIL'. If no specific solutions are mentioned and the response is focused on acknowledging the issue and gathering more information, return 'PASS'.\",\n",
      "      \"assertion_name\": \"no_specific_solutions\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"The task requires that the first response does not provide specific solutions to the user's issue but should acknowledge the issue and seek more information instead. The given output includes a greeting and an apology for the issue. It asks the user for their order number to investigate further without suggesting any immediate solutions or instructions.\"\n",
      "    },\n",
      "    \"Open-Ended Question\": {\n",
      "      \"assertion\": \"Evaluate the response to ensure it concludes with an open-ended question. The response should encourage further dialogue by asking the user for additional information or clarification, facilitating a continued back-and-forth communication. If the response ends with such a question, return 'PASS'. If it does not, return 'FAIL'. Consider whether the question naturally invites further conversation rather than providing definitive answers.\",\n",
      "      \"assertion_name\": \"check_response_ends_with_open_ended_question\",\n",
      "      \"type\": \"llm\",\n",
      "      \"score\": 1,\n",
      "      \"result\": \"PASS\",\n",
      "      \"explanation\": \"The task requires evaluating the response based on whether it concludes with an open-ended question to encourage further dialogue. The provided response ends with the question: 'Could you please provide me with your order number so I can look into this for you?' This is an open-ended question as it inquires further information, facilitating continued back-and-forth communication.\"\n",
      "    },\n",
      "    \"Paragraph Length\": {\n",
      "      \"assertion\": \"def test_paragraph_brevity(self):\\n    # Split the output into sentences by checking the presence of periods.\\n    sentences = self.output.get('Output', '').split('. ')\\n    # Verify the length of the output is a short paragraph (4-6 sentences typical range for short paragraphs).\\n    self.assertTrue(4 <= len(sentences) <= 6, \\\"Response should be a short paragraph between 4 to 6 sentences.\\\")\",\n",
      "      \"assertion_name\": \"test_paragraph_brevity\",\n",
      "      \"type\": \"python\",\n",
      "      \"score\": 0,\n",
      "      \"result\": \"FAIL\",\n",
      "      \"explanation\": null\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluations\n",
    "# Note: This is a placeholder as we don't have the actual testcase and criterion_assertion_map\n",
    "# You would need to extract these from the evaluation_suite response\n",
    "run_evaluations_response = requests.post(\n",
    "    run_evaluations_url,\n",
    "    json={\n",
    "        \"testcase\": {\n",
    "            \"input\": \"My order hasn't arrived yet. It's been a week.\",\n",
    "            \"output\": \"Hello there! I'm sorry to hear that your order hasn't arrived yet. That must be frustrating. I'd like to help you track down your package. Could you please provide me with your order number so I can look into this for you?\"\n",
    "        },\n",
    "        \"evaluation_suite\": evaluation_suite\n",
    "    }\n",
    ")\n",
    "\n",
    "if run_evaluations_response.status_code == 200:\n",
    "    evaluation_results = run_evaluations_response.json()\n",
    "    print(\"Evaluations run successfully.\")\n",
    "    print(json.dumps(evaluation_results, indent=2))\n",
    "else:\n",
    "    print(f\"Evaluations failed with status code: {run_evaluations_response.status_code}\")\n",
    "    print(run_evaluations_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suite_description': 'The task is to generate an initial response to a customer support inquiry. The response must include a warm greeting, acknowledgment of the issue, and an open-ended question for further engagement. The response should be professionally friendly, concise, and should not offer specific solutions in this initial message.',\n",
       " 'evaluation_criteria': [{'criterion': 'Warm Greeting',\n",
       "   'explaination': 'The response should start with a warm greeting to make the customer feel valued and acknowledged.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Acknowledgement of the Issue',\n",
       "   'explaination': \"Clearly acknowledging the customer's issue shows empathy and assures the customer that their concern is being taken seriously.\",\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Professional and Friendly Tone',\n",
       "   'explaination': 'A professional yet friendly tone is essential to ensure the customer feels comfortable and respected.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Conciseness',\n",
       "   'explaination': 'The response should be limited to one short paragraph, making it straightforward and easy to read.',\n",
       "   'evaluation_method': 'code'},\n",
       "  {'criterion': 'Omission of Specific Solutions',\n",
       "   'explaination': 'Specific solutions should not be provided in the first response to encourage further dialogue and information gathering.',\n",
       "   'evaluation_method': 'llm'},\n",
       "  {'criterion': 'Open-ended Question',\n",
       "   'explaination': 'Ending with an open-ended question encourages the customer to provide more information, keeping the conversation active.',\n",
       "   'evaluation_method': 'llm'}],\n",
       " 'data_generation_scenarios': [{'scenarios_based_on': \"User's Issue Type\",\n",
       "   'scenarios': ['Shipping delay',\n",
       "    'Product issue',\n",
       "    'Billing error',\n",
       "    'Account support']},\n",
       "  {'scenarios_based_on': \"User's Information Availability\",\n",
       "   'scenarios': ['User provides name', 'User does not provide name']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"detail\":[{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Warm Greeting\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"warm_greeting_presence\",\"text\":\"Evaluate if the response starts with a warm greeting. A warm greeting should include a friendly salutation (e.g., 'Hello', 'Hi') and should express a positive tone to help de-escalate any initial frustration from the user. If the response begins with such a warm greeting, return 'PASS'. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Acknowledgment of Issue\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"acknowledgment_of_issue\",\"text\":\"Evaluate whether the response explicitly acknowledges the user's reported issue. Look for phrases or sentences in the response that directly refer to the issue, such as a delayed shipment, in a way that shows clear understanding and acknowledgment of the user's situation. Return 'PASS' if the acknowledgment is present and clear; otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Professional Yet Friendly Tone\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"professional_yet_friendly_tone\",\"text\":\"Evaluate whether the LLM response maintains a professional yet friendly tone. The response should be respectful, empathetic, and polite, while also being approachable and warm. Consider whether the language used makes the user feel valued and respected. Return 'PASS' if the response maintains this balance, otherwise return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Limitation to One Short Paragraph\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"test_response_limited_to_one_paragraph\",\"code\":\"def test_response_limited_to_one_paragraph(self):\\n    # Ensure the response is limited to one paragraph\\n    response_text = self.output['text']\\n    # Splitting the text by new lines to count paragraphs\\n    paragraphs = response_text.strip().split('\\\\n')\\n    # Check that there is only one non-empty paragraph\\n    self.assertEqual(sum(1 for paragraph in paragraphs if paragraph.strip()), 1, \\\"The response should be limited to one short paragraph.\\\")\",\"evaluation_type\":\"python\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Avoiding Specific Solutions\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"avoiding_specific_solutions\",\"text\":\"Evaluate the response provided and determine if it avoids giving specific solutions regarding the user's issue. The response should focus on acknowledging the user's concern and gathering more information without suggesting concrete steps or solutions in the first reply. If the response refrains from offering specific solutions and instead asks for additional information to proceed, mark it as 'PASS'. Otherwise, mark it as 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Using User's Name\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"test_use_of_user_name\",\"code\":\"def test_use_of_user_name(self):\\n    # Check if the user's name is included\\n    user_name_placeholder = \\\"[User]\\\"\\n    output_text = self.output.get('Output', '')\\n    self.assertIn(user_name_placeholder, output_text, \\\"The response should include the user's name to add a personal touch.\\\")\",\"evaluation_type\":\"python\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"},{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"criterion_assertion_map\",\"Ending with Open-Ended Question\"],\"msg\":\"Extra inputs are not permitted\",\"input\":[{\"test_name\":\"evaluate_ending_with_open_ended_question\",\"text\":\"Given the LLM's response to a customer support inquiry, evaluate if the response concludes with an open-ended question designed to encourage further dialogue. The question should solicit additional information or engagement from the user rather than a simple 'yes' or 'no' answer. Return 'PASS' if the response ends in an open-ended question that invites user interaction and aligns with this criterion. Otherwise, return 'FAIL'.\",\"evaluation_type\":\"llm\"}],\"url\":\"https://errors.pydantic.dev/2.9/v/extra_forbidden\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /generate_examples (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x175d8dd90>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x175d8dd90>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /generate_examples (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x175d8dd90>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run evaluations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Note: This is a placeholder as we don't have the actual testcase and criterion_assertion_map\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# You would need to extract these from the evaluation_suite response\u001b[39;00m\n\u001b[1;32m      4\u001b[0m new_examples_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8000/generate_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m new_examples_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_examples_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation_suite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_suite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_examples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdd an example where the agent uses the user\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_examples_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     15\u001b[0m     new_examples \u001b[38;5;241m=\u001b[39m new_examples_response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/code/judge-tuner/.venv/lib/python3.12/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /generate_examples (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x175d8dd90>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# Run evaluations\n",
    "# Note: This is a placeholder as we don't have the actual testcase and criterion_assertion_map\n",
    "# You would need to extract these from the evaluation_suite response\n",
    "new_examples_url = \"http://localhost:8000/generate_examples\"\n",
    "new_examples_response = requests.post(\n",
    "    new_examples_url,\n",
    "    json={\n",
    "        \"evaluation_suite\": evaluation_suite,\n",
    "        \"num_examples\": 1,\n",
    "        \"note\": \"Add an example where the agent uses the user's name\"\n",
    "    }\n",
    ")\n",
    "\n",
    "if new_examples_response.status_code == 200:\n",
    "    new_examples = new_examples_response.json()\n",
    "    print(\"New examples generated successfully.\")\n",
    "    print(json.dumps(new_examples, indent=2))\n",
    "else:\n",
    "    print(f\"Evaluations failed with status code: {run_evaluations_response.status_code}\")\n",
    "    print(run_evaluations_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
